"""
í…”ë ˆê·¸ë¨, ê³ ë˜ ê±°ë˜, íŠ¸ìœ„í„° ì¸í”Œë£¨ì–¸ì„œ ë°ì´í„° í†µí•© ìƒê´€ê´€ê³„ ë¶„ì„

ì´ ëª¨ë“ˆì€ ì„¸ ê°€ì§€ ë°ì´í„° ì†ŒìŠ¤ë¥¼ í†µí•©í•˜ì—¬:
1. ì‹œê°„ë³„ ë°ì´í„° ë™ê¸°í™”
2. êµì°¨ ìƒê´€ê´€ê³„ ë¶„ì„
3. í†µí•© ìŠ¤íŒŒì´í¬ ê°ì§€
4. ì•ŒëŒ ìƒì„± ë° ìš°ì„ ìˆœìœ„ ì„¤ì •
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from scipy import stats
from scipy.stats import pearsonr, spearmanr
import warnings
warnings.filterwarnings('ignore')


class MultiSourceCorrelationAnalyzer:
    """ë‹¤ì¤‘ ì†ŒìŠ¤ ìƒê´€ê´€ê³„ ë¶„ì„ í´ë˜ìŠ¤"""
    
    def __init__(self, telegram_path, whale_path, twitter_path):
        """
        Args:
            telegram_path: í…”ë ˆê·¸ë¨ ë°ì´í„° CSV ê²½ë¡œ
            whale_path: ê³ ë˜ ê±°ë˜ ë°ì´í„° CSV ê²½ë¡œ
            twitter_path: íŠ¸ìœ„í„° ì¸í”Œë£¨ì–¸ì„œ ë°ì´í„° CSV ê²½ë¡œ
        """
        print("ë°ì´í„° ë¡œë”© ì¤‘...")
        
        # í…”ë ˆê·¸ë¨ ë°ì´í„° ë¡œë“œ
        self.telegram_df = pd.read_csv(telegram_path)
        self.telegram_df['timestamp'] = pd.to_datetime(self.telegram_df['timestamp'], utc=True).dt.tz_localize(None)
        print(f"âœ“ í…”ë ˆê·¸ë¨ ë°ì´í„°: {len(self.telegram_df)} rows")
        
        # ê³ ë˜ ê±°ë˜ ë°ì´í„° ë¡œë“œ
        self.whale_df = pd.read_csv(whale_path)
        # ì˜ëª»ëœ íƒ€ì„ìŠ¤íƒ¬í”„ í•„í„°ë§ (errors='coerce'ë¡œ ë³€í™˜ ì‹¤íŒ¨ ì‹œ NaTë¡œ ì²˜ë¦¬)
        self.whale_df['timestamp'] = pd.to_datetime(self.whale_df['block_timestamp'], errors='coerce')
        # NaT ì œê±° ë° íƒ€ì„ì¡´ ì œê±°
        self.whale_df = self.whale_df.dropna(subset=['timestamp'])
        if self.whale_df['timestamp'].dt.tz is not None:
            self.whale_df['timestamp'] = self.whale_df['timestamp'].dt.tz_localize(None)
        print(f"âœ“ ê³ ë˜ ê±°ë˜ ë°ì´í„°: {len(self.whale_df)} rows (ìœ íš¨í•œ íƒ€ì„ìŠ¤íƒ¬í”„ë§Œ)")
        
        # íŠ¸ìœ„í„° ì¸í”Œë£¨ì–¸ì„œ ë°ì´í„° ë¡œë“œ
        self.twitter_df = pd.read_csv(twitter_path)
        # ì˜ëª»ëœ íƒ€ì„ìŠ¤íƒ¬í”„ í•„í„°ë§
        self.twitter_df['timestamp'] = pd.to_datetime(self.twitter_df['post_date'], errors='coerce')
        # NaT ì œê±° ë° íƒ€ì„ì¡´ ì œê±°
        self.twitter_df = self.twitter_df.dropna(subset=['timestamp'])
        if self.twitter_df['timestamp'].dt.tz is not None:
            self.twitter_df['timestamp'] = self.twitter_df['timestamp'].dt.tz_localize(None)
        print(f"âœ“ íŠ¸ìœ„í„° ì¸í”Œë£¨ì–¸ì„œ ë°ì´í„°: {len(self.twitter_df)} rows (ìœ íš¨í•œ íƒ€ì„ìŠ¤íƒ¬í”„ë§Œ)")
        
        self.merged_df = None
        
    def preprocess_telegram_data(self, freq='1H'):
        """
        í…”ë ˆê·¸ë¨ ë°ì´í„° ì „ì²˜ë¦¬ ë° ì‹œê°„ë³„ ì§‘ê³„
        
        Args:
            freq: ì§‘ê³„ ì£¼ê¸° (ê¸°ë³¸: 1ì‹œê°„)
            
        Returns:
            DataFrame: ì‹œê°„ë³„ ì§‘ê³„ ë°ì´í„°
        """
        df = self.telegram_df.copy()
        
        # ì‹œê°„ë³„ ì§‘ê³„
        df_grouped = df.groupby(pd.Grouper(key='timestamp', freq=freq)).agg({
            'message_count': 'sum',
            'avg_views': 'mean',
            'total_forwards': 'sum',
            'total_reactions': 'sum',
            'avg_sentiment': 'mean',
            'avg_positive': 'mean',
            'avg_negative': 'mean',
            'avg_neutral': 'mean',
            'avg_msg_length': 'mean'
        }).reset_index()
        
        # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
        df_grouped = df_grouped.fillna(0)
        
        # ì»¬ëŸ¼ ì´ë¦„ ë³€ê²½ (ì ‘ë‘ì‚¬ ì¶”ê°€)
        df_grouped.columns = ['timestamp'] + [f'telegram_{col}' for col in df_grouped.columns[1:]]
        
        return df_grouped
    
    def preprocess_whale_data(self, freq='1H'):
        """
        ê³ ë˜ ê±°ë˜ ë°ì´í„° ì „ì²˜ë¦¬ ë° ì‹œê°„ë³„ ì§‘ê³„
        
        Args:
            freq: ì§‘ê³„ ì£¼ê¸° (ê¸°ë³¸: 1ì‹œê°„)
            
        Returns:
            DataFrame: ì‹œê°„ë³„ ì§‘ê³„ ë°ì´í„°
        """
        df = self.whale_df.copy()
        
        # amount ì»¬ëŸ¼ì„ ìˆ«ìë¡œ ë³€í™˜
        df['amount'] = pd.to_numeric(df['amount'], errors='coerce')
        
        # ì‹œê°„ë³„ ì§‘ê³„
        df_grouped = df.groupby(pd.Grouper(key='timestamp', freq=freq)).agg({
            'tx_hash': 'count',  # ê±°ë˜ ë¹ˆë„
            'amount': ['sum', 'mean', 'max', 'std'],  # ê±°ë˜ëŸ‰ í†µê³„
            'coin_symbol': lambda x: x.mode()[0] if len(x.mode()) > 0 else 'ETH'  # ì£¼ìš” ì½”ì¸
        }).reset_index()
        
        # ì»¬ëŸ¼ ì´ë¦„ í‰íƒ„í™”
        df_grouped.columns = ['timestamp', 'whale_tx_count', 'whale_volume_sum', 
                              'whale_volume_mean', 'whale_volume_max', 
                              'whale_volume_std', 'whale_main_coin']
        
        # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
        df_grouped[['whale_volume_sum', 'whale_volume_mean', 'whale_volume_max', 'whale_volume_std']] = \
            df_grouped[['whale_volume_sum', 'whale_volume_mean', 'whale_volume_max', 'whale_volume_std']].fillna(0)
        
        return df_grouped
    
    def preprocess_twitter_data(self, freq='1H'):
        """
        íŠ¸ìœ„í„° ì¸í”Œë£¨ì–¸ì„œ ë°ì´í„° ì „ì²˜ë¦¬ ë° ì‹œê°„ë³„ ì§‘ê³„
        
        Args:
            freq: ì§‘ê³„ ì£¼ê¸° (ê¸°ë³¸: 1ì‹œê°„)
            
        Returns:
            DataFrame: ì‹œê°„ë³„ ì§‘ê³„ ë°ì´í„°
        """
        df = self.twitter_df.copy()
        
        # ìˆ«ì ì»¬ëŸ¼ ë³€í™˜
        df['likes'] = pd.to_numeric(df['likes'], errors='coerce').fillna(0)
        df['shares'] = pd.to_numeric(df['shares'], errors='coerce').fillna(0)
        df['comments'] = pd.to_numeric(df['comments'], errors='coerce').fillna(0)
        df['sentiment_score'] = pd.to_numeric(df['sentiment_score'], errors='coerce').fillna(0)
        
        # ì¸ê²Œì´ì§€ë¨¼íŠ¸ ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘ì¹˜ ì ìš©)
        df['engagement_score'] = df['likes'] * 1 + df['shares'] * 3 + df['comments'] * 2
        
        # ì‹œê°„ë³„ ì§‘ê³„
        df_grouped = df.groupby(pd.Grouper(key='timestamp', freq=freq)).agg({
            'post_url': 'count',  # í¬ìŠ¤íŠ¸ ìˆ˜
            'likes': 'sum',
            'shares': 'sum',
            'comments': 'sum',
            'engagement_score': 'sum',
            'sentiment_score': 'mean',
            'spc_coin_label': lambda x: (x != 0).sum()  # ì½”ì¸ ì–¸ê¸‰ ìˆ˜
        }).reset_index()
        
        # ì»¬ëŸ¼ ì´ë¦„ ë³€ê²½
        df_grouped.columns = ['timestamp', 'twitter_post_count', 'twitter_likes', 
                              'twitter_shares', 'twitter_comments', 
                              'twitter_engagement', 'twitter_sentiment',
                              'twitter_coin_mentions']
        
        # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
        df_grouped = df_grouped.fillna(0)
        
        return df_grouped
    
    def merge_all_data(self, freq='1H'):
        """
        ëª¨ë“  ë°ì´í„° ì†ŒìŠ¤ë¥¼ ì‹œê°„ë³„ë¡œ ë³‘í•©
        
        Args:
            freq: ì§‘ê³„ ì£¼ê¸° (ê¸°ë³¸: 1ì‹œê°„)
            
        Returns:
            DataFrame: ë³‘í•©ëœ ë°ì´í„°
        """
        print(f"\në°ì´í„° ë³‘í•© ì¤‘ (ì£¼ê¸°: {freq})...")
        
        # ê° ë°ì´í„° ì „ì²˜ë¦¬
        telegram_processed = self.preprocess_telegram_data(freq)
        whale_processed = self.preprocess_whale_data(freq)
        twitter_processed = self.preprocess_twitter_data(freq)
        
        # ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ë³‘í•© (outer joinìœ¼ë¡œ ëª¨ë“  ì‹œê°„ëŒ€ í¬í•¨)
        merged = telegram_processed.merge(whale_processed, on='timestamp', how='outer')
        merged = merged.merge(twitter_processed, on='timestamp', how='outer')
        
        # ê²°ì¸¡ì¹˜ë¥¼ 0ìœ¼ë¡œ ì±„ìš°ê¸°
        merged = merged.fillna(0)
        
        # ì‹œê°„ìˆœ ì •ë ¬
        merged = merged.sort_values('timestamp').reset_index(drop=True)
        
        self.merged_df = merged
        
        print(f"âœ“ ë³‘í•© ì™„ë£Œ: {len(merged)} rows, {len(merged.columns)} columns")
        print(f"  ì‹œê°„ ë²”ìœ„: {merged['timestamp'].min()} ~ {merged['timestamp'].max()}")
        
        return merged
    
    def calculate_correlations(self, method='pearson'):
        """
        ëª¨ë“  ë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„ ê³„ì‚°
        
        Args:
            method: 'pearson' ë˜ëŠ” 'spearman'
            
        Returns:
            DataFrame: ìƒê´€ê³„ìˆ˜ ë§¤íŠ¸ë¦­ìŠ¤
        """
        if self.merged_df is None:
            raise ValueError("ë¨¼ì € merge_all_data()ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        
        # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë§Œ ì„ íƒ (timestamp ì œì™¸)
        numeric_cols = self.merged_df.select_dtypes(include=[np.number]).columns.tolist()
        
        # ìƒê´€ê³„ìˆ˜ ê³„ì‚°
        if method == 'pearson':
            corr_matrix = self.merged_df[numeric_cols].corr(method='pearson')
        else:
            corr_matrix = self.merged_df[numeric_cols].corr(method='spearman')
        
        return corr_matrix
    
    def find_significant_correlations(self, threshold=0.3, p_value_threshold=0.05):
        """
        í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ ìƒê´€ê´€ê³„ ì°¾ê¸°
        
        Args:
            threshold: ìƒê´€ê³„ìˆ˜ ì„ê³„ê°’ (ì ˆëŒ€ê°’)
            p_value_threshold: p-value ì„ê³„ê°’
            
        Returns:
            DataFrame: ìœ ì˜ë¯¸í•œ ìƒê´€ê´€ê³„ ë¦¬ìŠ¤íŠ¸
        """
        if self.merged_df is None:
            raise ValueError("ë¨¼ì € merge_all_data()ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        
        numeric_cols = self.merged_df.select_dtypes(include=[np.number]).columns.tolist()
        
        results = []
        
        # ëª¨ë“  ì»¬ëŸ¼ ìŒì— ëŒ€í•´ ìƒê´€ê´€ê³„ ê²€ì •
        for i, col1 in enumerate(numeric_cols):
            for col2 in numeric_cols[i+1:]:
                # 0ì´ ì•„ë‹Œ ê°’ì´ ì¶©ë¶„íˆ ìˆëŠ”ì§€ í™•ì¸
                valid_data = self.merged_df[[col1, col2]].dropna()
                valid_data = valid_data[(valid_data[col1] != 0) | (valid_data[col2] != 0)]
                
                if len(valid_data) > 10:  # ìµœì†Œ ë°ì´í„° í¬ì¸íŠ¸
                    try:
                        # Pearson ìƒê´€ê³„ìˆ˜ ë° p-value
                        corr, p_value = pearsonr(valid_data[col1], valid_data[col2])
                        
                        # ì„ê³„ê°’ ì²´í¬
                        if abs(corr) >= threshold and p_value < p_value_threshold:
                            results.append({
                                'variable_1': col1,
                                'variable_2': col2,
                                'correlation': corr,
                                'p_value': p_value,
                                'n_samples': len(valid_data),
                                'significance': 'high' if p_value < 0.01 else 'medium'
                            })
                    except:
                        pass
        
        results_df = pd.DataFrame(results)
        
        if not results_df.empty:
            # ìƒê´€ê³„ìˆ˜ ì ˆëŒ€ê°’ìœ¼ë¡œ ì •ë ¬
            results_df = results_df.sort_values('correlation', key=lambda x: x.abs(), ascending=False)
        
        return results_df
    
    def analyze_cross_source_patterns(self):
        """
        ì†ŒìŠ¤ ê°„ êµì°¨ íŒ¨í„´ ë¶„ì„
        íŠ¹ì • ê´€ì‹¬ ìˆëŠ” íŒ¨í„´:
        1. í…”ë ˆê·¸ë¨ í™œë™ ì¦ê°€ â†’ ê³ ë˜ ê±°ë˜ ì¦ê°€
        2. íŠ¸ìœ„í„° ì¸í”Œë£¨ì–¸ì„œ í™œë™ â†’ ê³ ë˜ ê±°ë˜
        3. ì„¸ ì†ŒìŠ¤ ëª¨ë‘ ë™ì‹œ ê¸‰ì¦
        
        Returns:
            dict: íŒ¨í„´ ë¶„ì„ ê²°ê³¼
        """
        if self.merged_df is None:
            raise ValueError("ë¨¼ì € merge_all_data()ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        
        df = self.merged_df.copy()
        
        patterns = {}
        
        # íŒ¨í„´ 1: í…”ë ˆê·¸ë¨ â†’ ê³ ë˜ ê±°ë˜
        if 'telegram_message_count' in df.columns and 'whale_tx_count' in df.columns:
            # ì‹œì°¨ ìƒê´€ê´€ê³„ (lag correlation)
            lags = range(0, 24)  # 0~23ì‹œê°„ ì‹œì°¨
            lag_corrs = []
            
            for lag in lags:
                if lag == 0:
                    shifted_whale = df['whale_tx_count']
                else:
                    shifted_whale = df['whale_tx_count'].shift(-lag)
                
                valid_data = pd.DataFrame({
                    'telegram': df['telegram_message_count'],
                    'whale': shifted_whale
                }).dropna()
                
                if len(valid_data) > 10:
                    corr, p_val = pearsonr(valid_data['telegram'], valid_data['whale'])
                    lag_corrs.append({'lag_hours': lag, 'correlation': corr, 'p_value': p_val})
            
            patterns['telegram_to_whale'] = pd.DataFrame(lag_corrs)
        
        # íŒ¨í„´ 2: íŠ¸ìœ„í„° â†’ ê³ ë˜ ê±°ë˜
        if 'twitter_engagement' in df.columns and 'whale_tx_count' in df.columns:
            lags = range(0, 24)
            lag_corrs = []
            
            for lag in lags:
                if lag == 0:
                    shifted_whale = df['whale_tx_count']
                else:
                    shifted_whale = df['whale_tx_count'].shift(-lag)
                
                valid_data = pd.DataFrame({
                    'twitter': df['twitter_engagement'],
                    'whale': shifted_whale
                }).dropna()
                
                if len(valid_data) > 10:
                    corr, p_val = pearsonr(valid_data['twitter'], valid_data['whale'])
                    lag_corrs.append({'lag_hours': lag, 'correlation': corr, 'p_value': p_val})
            
            patterns['twitter_to_whale'] = pd.DataFrame(lag_corrs)
        
        # íŒ¨í„´ 3: ë™ì‹œ ê¸‰ì¦ ì´ë²¤íŠ¸ ê°ì§€
        # Z-scoreë¡œ ì •ê·œí™”í•˜ì—¬ ê¸‰ì¦ íŒë‹¨
        spike_threshold = 2.0
        
        for col in ['telegram_message_count', 'whale_tx_count', 'twitter_engagement']:
            if col in df.columns:
                mean = df[col].rolling(window=24, min_periods=1).mean()
                std = df[col].rolling(window=24, min_periods=1).std()
                df[f'{col}_zscore'] = (df[col] - mean) / (std + 1e-10)
        
        # ë™ì‹œ ê¸‰ì¦ ì¡°ê±´
        if all(f'{col}_zscore' in df.columns for col in ['telegram_message_count', 'whale_tx_count', 'twitter_engagement']):
            df['triple_spike'] = (
                (df['telegram_message_count_zscore'] > spike_threshold) &
                (df['whale_tx_count_zscore'] > spike_threshold) &
                (df['twitter_engagement_zscore'] > spike_threshold)
            )
            
            triple_spike_events = df[df['triple_spike']].copy()
            patterns['triple_spike_events'] = triple_spike_events
            
        # ì´ì¤‘ ê¸‰ì¦ (í…”ë ˆê·¸ë¨ + ê³ ë˜)
        if 'telegram_message_count_zscore' in df.columns and 'whale_tx_count_zscore' in df.columns:
            df['telegram_whale_spike'] = (
                (df['telegram_message_count_zscore'] > spike_threshold) &
                (df['whale_tx_count_zscore'] > spike_threshold)
            )
            
            telegram_whale_events = df[df['telegram_whale_spike']].copy()
            patterns['telegram_whale_spike_events'] = telegram_whale_events
        
        # ì´ì¤‘ ê¸‰ì¦ (íŠ¸ìœ„í„° + ê³ ë˜)
        if 'twitter_engagement_zscore' in df.columns and 'whale_tx_count_zscore' in df.columns:
            df['twitter_whale_spike'] = (
                (df['twitter_engagement_zscore'] > spike_threshold) &
                (df['whale_tx_count_zscore'] > spike_threshold)
            )
            
            twitter_whale_events = df[df['twitter_whale_spike']].copy()
            patterns['twitter_whale_spike_events'] = twitter_whale_events
        
        self.merged_df = df  # ì—…ë°ì´íŠ¸ëœ ë°ì´í„° ì €ì¥
        
        return patterns
    
    def generate_alert_priority(self, row):
        """
        í–‰ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì•ŒëŒ ìš°ì„ ìˆœìœ„ ê³„ì‚°
        
        Args:
            row: DataFrameì˜ í•œ í–‰
            
        Returns:
            dict: ì•ŒëŒ ì •ë³´
        """
        priority_score = 0
        alert_reasons = []
        
        # í…”ë ˆê·¸ë¨ ìŠ¤íŒŒì´í¬
        if 'telegram_message_count_zscore' in row and row['telegram_message_count_zscore'] > 2.0:
            priority_score += 2
            alert_reasons.append(f"í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ê¸‰ì¦ (z={row['telegram_message_count_zscore']:.2f})")
        
        # ê³ ë˜ ê±°ë˜ ìŠ¤íŒŒì´í¬
        if 'whale_tx_count_zscore' in row and row['whale_tx_count_zscore'] > 2.0:
            priority_score += 3  # ê³ ë˜ ê±°ë˜ê°€ ë” ì¤‘ìš”
            alert_reasons.append(f"ê³ ë˜ ê±°ë˜ ê¸‰ì¦ (z={row['whale_tx_count_zscore']:.2f})")
        
        # íŠ¸ìœ„í„° ì¸ê²Œì´ì§€ë¨¼íŠ¸ ìŠ¤íŒŒì´í¬
        if 'twitter_engagement_zscore' in row and row['twitter_engagement_zscore'] > 2.0:
            priority_score += 2
            alert_reasons.append(f"íŠ¸ìœ„í„° ì¸í”Œë£¨ì–¸ì„œ í™œë™ ê¸‰ì¦ (z={row['twitter_engagement_zscore']:.2f})")
        
        # ë³µí•© ìŠ¤íŒŒì´í¬ ë³´ë„ˆìŠ¤
        if 'telegram_whale_spike' in row and row['telegram_whale_spike']:
            priority_score += 5
            alert_reasons.append("âš ï¸ í…”ë ˆê·¸ë¨+ê³ ë˜ ë™ì‹œ ê¸‰ì¦")
        
        if 'twitter_whale_spike' in row and row['twitter_whale_spike']:
            priority_score += 4
            alert_reasons.append("âš ï¸ íŠ¸ìœ„í„°+ê³ ë˜ ë™ì‹œ ê¸‰ì¦")
        
        if 'triple_spike' in row and row['triple_spike']:
            priority_score += 10
            alert_reasons.append("ğŸš¨ 3ê°œ ì†ŒìŠ¤ ëª¨ë‘ ê¸‰ì¦ (CRITICAL)")
        
        # ìš°ì„ ìˆœìœ„ ë ˆë²¨ ê²°ì •
        if priority_score >= 10:
            level = 'CRITICAL'
        elif priority_score >= 5:
            level = 'HIGH'
        elif priority_score >= 2:
            level = 'MEDIUM'
        else:
            level = 'LOW'
        
        return {
            'priority_score': priority_score,
            'alert_level': level,
            'reasons': '; '.join(alert_reasons)
        }
    
    def generate_all_alerts(self, min_priority_score=2):
        """
        ëª¨ë“  ì•ŒëŒ ìƒì„±
        
        Args:
            min_priority_score: ìµœì†Œ ìš°ì„ ìˆœìœ„ ì ìˆ˜
            
        Returns:
            DataFrame: ì•ŒëŒ ë°ì´í„°
        """
        if self.merged_df is None:
            raise ValueError("ë¨¼ì € merge_all_data()ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        
        alerts = []
        
        for idx, row in self.merged_df.iterrows():
            alert_info = self.generate_alert_priority(row)
            
            if alert_info['priority_score'] >= min_priority_score:
                alerts.append({
                    'timestamp': row['timestamp'],
                    'priority_score': alert_info['priority_score'],
                    'alert_level': alert_info['alert_level'],
                    'reasons': alert_info['reasons'],
                    'telegram_msgs': row.get('telegram_message_count', 0),
                    'whale_txs': row.get('whale_tx_count', 0),
                    'twitter_engagement': row.get('twitter_engagement', 0),
                    'telegram_sentiment': row.get('telegram_avg_sentiment', 0),
                    'twitter_sentiment': row.get('twitter_sentiment', 0),
                })
        
        alerts_df = pd.DataFrame(alerts)
        
        if not alerts_df.empty:
            # ìš°ì„ ìˆœìœ„ ì ìˆ˜ë¡œ ì •ë ¬
            alerts_df = alerts_df.sort_values(['priority_score', 'timestamp'], ascending=[False, False])
        
        return alerts_df
    
    def save_results(self, output_dir='/Volumes/T7/class/2025-FALL/big_data/data'):
        """
        ë¶„ì„ ê²°ê³¼ ì €ì¥
        
        Args:
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        """
        import os
        
        # ë³‘í•© ë°ì´í„° ì €ì¥
        if self.merged_df is not None:
            output_path = os.path.join(output_dir, 'multi_source_merged_data.csv')
            self.merged_df.to_csv(output_path, index=False)
            print(f"âœ“ ë³‘í•© ë°ì´í„° ì €ì¥: {output_path}")
        
        # ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤ ì €ì¥
        corr_matrix = self.calculate_correlations()
        corr_path = os.path.join(output_dir, 'multi_source_correlation_matrix.csv')
        corr_matrix.to_csv(corr_path)
        print(f"âœ“ ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤ ì €ì¥: {corr_path}")
        
        # ìœ ì˜ë¯¸í•œ ìƒê´€ê´€ê³„ ì €ì¥
        sig_corr = self.find_significant_correlations()
        if not sig_corr.empty:
            sig_path = os.path.join(output_dir, 'multi_source_significant_correlations.csv')
            sig_corr.to_csv(sig_path, index=False)
            print(f"âœ“ ìœ ì˜ë¯¸í•œ ìƒê´€ê´€ê³„ ì €ì¥: {sig_path}")
        
        # ì•ŒëŒ ìƒì„± ë° ì €ì¥
        alerts = self.generate_all_alerts(min_priority_score=2)
        if not alerts.empty:
            alerts_path = os.path.join(output_dir, 'multi_source_alerts.csv')
            alerts.to_csv(alerts_path, index=False)
            print(f"âœ“ ì•ŒëŒ ë°ì´í„° ì €ì¥: {alerts_path}")
        
        # íŒ¨í„´ ë¶„ì„ ê²°ê³¼ ì €ì¥
        patterns = self.analyze_cross_source_patterns()
        
        for pattern_name, pattern_data in patterns.items():
            if isinstance(pattern_data, pd.DataFrame) and not pattern_data.empty:
                pattern_path = os.path.join(output_dir, f'pattern_{pattern_name}.csv')
                pattern_data.to_csv(pattern_path, index=False)
                print(f"âœ“ íŒ¨í„´ ë¶„ì„ ì €ì¥: {pattern_path}")


if __name__ == '__main__':
    # ë°ì´í„° ê²½ë¡œ
    TELEGRAM_PATH = '/Volumes/T7/class/2025-FALL/big_data/data/telegram_data.csv'
    WHALE_PATH = '/Volumes/T7/class/2025-FALL/big_data/data/whale_transactions_rows.csv'
    TWITTER_PATH = '/Volumes/T7/class/2025-FALL/big_data/data/twitter_influencer_labeled_rows.csv'
    
    print("=" * 80)
    print("ë‹¤ì¤‘ ì†ŒìŠ¤ ìƒê´€ê´€ê³„ ë¶„ì„ ì‹œì‘")
    print("=" * 80)
    
    # ë¶„ì„ê¸° ì´ˆê¸°í™”
    analyzer = MultiSourceCorrelationAnalyzer(TELEGRAM_PATH, WHALE_PATH, TWITTER_PATH)
    
    # ë°ì´í„° ë³‘í•©
    merged_df = analyzer.merge_all_data(freq='1H')
    
    print("\n" + "=" * 80)
    print("ìƒê´€ê´€ê³„ ë¶„ì„")
    print("=" * 80)
    
    # ìœ ì˜ë¯¸í•œ ìƒê´€ê´€ê³„ ì°¾ê¸°
    sig_corr = analyzer.find_significant_correlations(threshold=0.3, p_value_threshold=0.05)
    
    if not sig_corr.empty:
        print(f"\nâœ“ {len(sig_corr)}ê°œì˜ ìœ ì˜ë¯¸í•œ ìƒê´€ê´€ê³„ ë°œê²¬:")
        print(sig_corr.head(20).to_string())
    else:
        print("\nâš  ìœ ì˜ë¯¸í•œ ìƒê´€ê´€ê³„ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    
    print("\n" + "=" * 80)
    print("êµì°¨ íŒ¨í„´ ë¶„ì„")
    print("=" * 80)
    
    # íŒ¨í„´ ë¶„ì„
    patterns = analyzer.analyze_cross_source_patterns()
    
    for pattern_name, pattern_data in patterns.items():
        if isinstance(pattern_data, pd.DataFrame) and not pattern_data.empty:
            print(f"\n[{pattern_name}]")
            if 'lag_hours' in pattern_data.columns:
                # ì‹œì°¨ ìƒê´€ê´€ê³„
                max_corr = pattern_data.loc[pattern_data['correlation'].abs().idxmax()]
                print(f"  ìµœëŒ€ ìƒê´€ê´€ê³„: lag={max_corr['lag_hours']}h, corr={max_corr['correlation']:.3f}, p={max_corr['p_value']:.4f}")
            else:
                # ì´ë²¤íŠ¸ ìˆ˜
                print(f"  ê°ì§€ëœ ì´ë²¤íŠ¸: {len(pattern_data)}ê°œ")
                if len(pattern_data) > 0:
                    print(f"  ì²« ì´ë²¤íŠ¸: {pattern_data['timestamp'].iloc[0]}")
                    print(f"  ë§ˆì§€ë§‰ ì´ë²¤íŠ¸: {pattern_data['timestamp'].iloc[-1]}")
    
    print("\n" + "=" * 80)
    print("ì•ŒëŒ ìƒì„±")
    print("=" * 80)
    
    # ì•ŒëŒ ìƒì„±
    alerts = analyzer.generate_all_alerts(min_priority_score=2)
    
    if not alerts.empty:
        print(f"\nâœ“ {len(alerts)}ê°œì˜ ì•ŒëŒ ìƒì„±:")
        
        # ë ˆë²¨ë³„ ì¹´ìš´íŠ¸
        level_counts = alerts['alert_level'].value_counts()
        print(f"\në ˆë²¨ë³„ ë¶„í¬:")
        for level, count in level_counts.items():
            print(f"  {level}: {count}ê°œ")
        
        print(f"\nìƒìœ„ 10ê°œ ì•ŒëŒ:")
        print(alerts.head(10)[['timestamp', 'alert_level', 'priority_score', 'reasons']].to_string())
    else:
        print("\nâš  ìƒì„±ëœ ì•ŒëŒì´ ì—†ìŠµë‹ˆë‹¤.")
    
    print("\n" + "=" * 80)
    print("ê²°ê³¼ ì €ì¥")
    print("=" * 80)
    
    # ê²°ê³¼ ì €ì¥
    analyzer.save_results()
    
    print("\n" + "=" * 80)
    print("ë¶„ì„ ì™„ë£Œ!")
    print("=" * 80)

