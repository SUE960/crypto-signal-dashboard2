"""
ì½”ì¸ë‹ˆìŠ¤(Coinness) ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸

ì•”í˜¸í™”í ë‰´ìŠ¤ ì‚¬ì´íŠ¸ì—ì„œ ê¸°ì‚¬ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
"""

import os
import time
import random
from datetime import datetime, timedelta
import pandas as pd
import requests
from bs4 import BeautifulSoup
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

# ê°ì • ë¶„ì„ê¸° ì´ˆê¸°í™”
sentiment_analyzer = SentimentIntensityAnalyzer()

# User-Agent ë¦¬ìŠ¤íŠ¸ (ì›¹ ìŠ¤í¬ë˜í•‘ ë°©ì§€ ìš°íšŒ)
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15',
]


class CoinnessCollector:
    """ì½”ì¸ë‹ˆìŠ¤ ë‰´ìŠ¤ ìˆ˜ì§‘ê¸°"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.session = requests.Session()
        self.base_url = 'https://coinness.com'
        self.headers = {
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0',
        }
        self.retry_count = 3
        self.news_data = []
    
    def get_random_user_agent(self):
        """ëœë¤ User-Agent ë°˜í™˜"""
        return random.choice(USER_AGENTS)
    
    def fetch_page(self, url, page_num=1):
        """
        í˜ì´ì§€ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        
        Args:
            url: ìš”ì²­í•  URL
            page_num: í˜ì´ì§€ ë²ˆí˜¸
            
        Returns:
            BeautifulSoup ê°ì²´ ë˜ëŠ” None
        """
        for attempt in range(self.retry_count):
            try:
                # User-Agent ëœë¤ ë³€ê²½
                self.headers['User-Agent'] = self.get_random_user_agent()
                
                # ìš”ì²­ ì „ ì§€ì—° (1.5~3.5ì´ˆ)
                delay = random.uniform(1.5, 3.5)
                time.sleep(delay)
                
                response = self.session.get(
                    url,
                    headers=self.headers,
                    timeout=15
                )
                
                if response.status_code == 200:
                    return BeautifulSoup(response.text, 'html.parser')
                elif response.status_code == 429:
                    # Rate limiting - ë” ê¸´ ì§€ì—°
                    wait_time = random.uniform(10, 30)
                    print(f"  âš ï¸  Rate limit ê°ì§€. {wait_time:.1f}ì´ˆ ëŒ€ê¸° ì¤‘...")
                    time.sleep(wait_time)
                else:
                    print(f"  âš ï¸  í˜ì´ì§€ {page_num} ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")
                    
            except Exception as e:
                print(f"  âœ— í˜ì´ì§€ {page_num} ìš”ì²­ ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}/{self.retry_count}): {e}")
                
                if attempt < self.retry_count - 1:
                    wait_time = random.uniform(3, 8)
                    time.sleep(wait_time)
        
        return None
    
    def parse_news_article(self, article_elem):
        """
        ë‰´ìŠ¤ ê¸°ì‚¬ ìš”ì†Œë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤.
        
        Args:
            article_elem: BeautifulSoup ê¸°ì‚¬ ìš”ì†Œ (a íƒœê·¸)
            
        Returns:
            dict: íŒŒì‹±ëœ ë‰´ìŠ¤ ë°ì´í„° ë˜ëŠ” None
        """
        try:
            # ë§í¬ (a íƒœê·¸ ìì²´ê°€ ë§í¬)
            link = article_elem.get('href', '')
            
            # ì œëª© (h3 íƒœê·¸)
            title_elem = article_elem.find('h3', class_=lambda x: x and 'ArticleTitle' in x)
            if not title_elem:
                return None
            title = title_elem.get_text(strip=True)
            
            # ì‹œê°„ ì •ë³´ (TimeWrap ì•ˆì— ìˆìŒ)
            time_wrap = article_elem.find('div', class_=lambda x: x and 'TimeWrap' in x)
            if time_wrap:
                # time-badge (ì‹œê°„)
                time_badge = time_wrap.find('span', class_='time-badge')
                time_str = time_badge.get_text(strip=True) if time_badge else ''
                
                # ë‚ ì§œ (TimeWrapì˜ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ)
                date_text = time_wrap.get_text(strip=True)
                # "13:302025ë…„ 11ì›” 30ì¼ ì¼ìš”ì¼" í˜•íƒœì—ì„œ ë‚ ì§œ ì¶”ì¶œ
                date_parts = date_text.replace(time_str, '').strip()
            else:
                time_str = ''
                date_parts = ''
            
            # ì‹œê°„ íŒŒì‹±
            pub_time = self.parse_time_with_date(time_str, date_parts)
            
            # ë‚´ìš© ë¯¸ë¦¬ë³´ê¸° (p íƒœê·¸)
            content_elem = article_elem.find('p', class_=lambda x: x and 'ArticleDesc' in x)
            content = content_elem.get_text(strip=True) if content_elem else ''
            
            # ê°ì • ë¶„ì„ (ì œëª© + ë‚´ìš©)
            text_for_sentiment = f"{title} {content}"
            sentiment = sentiment_analyzer.polarity_scores(text_for_sentiment)
            
            return {
                'timestamp': pub_time,
                'title': title,
                'content': content,
                'link': link,
                'sentiment_compound': sentiment['compound'],
                'sentiment_positive': sentiment['pos'],
                'sentiment_negative': sentiment['neg'],
                'sentiment_neutral': sentiment['neu'],
            }
            
        except Exception as e:
            print(f"  âš ï¸  ê¸°ì‚¬ íŒŒì‹± ì˜¤ë¥˜: {e}")
            return None
    
    def parse_time_with_date(self, time_str, date_str):
        """
        ì‹œê°„ê³¼ ë‚ ì§œ ë¬¸ìì—´ì„ datetimeìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        
        Args:
            time_str: ì‹œê°„ ë¬¸ìì—´ (ì˜ˆ: "13:30")
            date_str: ë‚ ì§œ ë¬¸ìì—´ (ì˜ˆ: "2025ë…„ 11ì›” 30ì¼ ì¼ìš”ì¼")
            
        Returns:
            datetime: íŒŒì‹±ëœ ì‹œê°„
        """
        now = datetime.now()
        
        try:
            # ë‚ ì§œ íŒŒì‹± (ì˜ˆ: "2025ë…„ 11ì›” 30ì¼ ì¼ìš”ì¼")
            if 'ë…„' in date_str and 'ì›”' in date_str and 'ì¼' in date_str:
                # ë‚ ì§œì—ì„œ ìˆ«ìë§Œ ì¶”ì¶œ
                import re
                numbers = re.findall(r'\d+', date_str)
                
                if len(numbers) >= 3:
                    year = int(numbers[0])
                    month = int(numbers[1])
                    day = int(numbers[2])
                    
                    # ì‹œê°„ íŒŒì‹± (ì˜ˆ: "13:30")
                    if ':' in time_str:
                        time_parts = time_str.split(':')
                        hour = int(time_parts[0])
                        minute = int(time_parts[1]) if len(time_parts) > 1 else 0
                    else:
                        hour = 0
                        minute = 0
                    
                    return datetime(year, month, day, hour, minute)
            
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ í˜„ì¬ ì‹œê°„ ë°˜í™˜
            return now
                
        except Exception as e:
            print(f"  âš ï¸  ì‹œê°„ íŒŒì‹± ì˜¤ë¥˜: time={time_str}, date={date_str} - {e}")
            return now
    
    def collect_news(self, max_pages=50, start_date=None):
        """
        ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
        
        Args:
            max_pages: ìˆ˜ì§‘í•  ìµœëŒ€ í˜ì´ì§€ ìˆ˜
            start_date: ìˆ˜ì§‘ ì‹œì‘ ë‚ ì§œ (ì´ ë‚ ì§œ ì´í›„ì˜ ë‰´ìŠ¤ë§Œ ìˆ˜ì§‘)
            
        Returns:
            DataFrame: ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ë°ì´í„°
        """
        if start_date is None:
            start_date = datetime(2025, 1, 1)
        
        print(f"\nì½”ì¸ë‹ˆìŠ¤ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘...")
        print(f"  ìˆ˜ì§‘ ê¸°ê°„: {start_date.date()} ~ í˜„ì¬")
        print(f"  ìµœëŒ€ í˜ì´ì§€: {max_pages}")
        
        collected_count = 0
        
        for page in range(1, max_pages + 1):
            print(f"\ní˜ì´ì§€ {page}/{max_pages} ìˆ˜ì§‘ ì¤‘...")
            
            # í˜ì´ì§€ URL êµ¬ì„±
            url = f"{self.base_url}/article?page={page}"
            
            # í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
            soup = self.fetch_page(url, page)
            if not soup:
                print(f"  âœ— í˜ì´ì§€ {page} ë¡œë”© ì‹¤íŒ¨")
                continue
            
            # ë‰´ìŠ¤ ê¸°ì‚¬ ì°¾ê¸° (ì½”ì¸ë‹ˆìŠ¤ êµ¬ì¡°)
            # ArticleWrapper í´ë˜ìŠ¤ë¥¼ ê°€ì§„ a íƒœê·¸ë“¤
            articles = soup.find_all('a', class_=lambda x: x and 'ArticleWrapper' in x)
            
            if not articles:
                print(f"  âš ï¸  í˜ì´ì§€ {page}ì—ì„œ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                # ì²« í˜ì´ì§€ì—ì„œë„ ì°¾ì§€ ëª»í•˜ë©´ ì¤‘ë‹¨
                if page == 1:
                    print(f"  ğŸ’¡ HTML êµ¬ì¡° í™•ì¸ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                    break
                continue
            
            page_count = 0
            stop_collecting = False
            
            for article in articles:
                news_data = self.parse_news_article(article)
                
                if news_data:
                    # ë‚ ì§œ í•„í„°ë§
                    if news_data['timestamp'] < start_date:
                        stop_collecting = True
                        break
                    
                    self.news_data.append(news_data)
                    page_count += 1
                    collected_count += 1
            
            print(f"  âœ“ í˜ì´ì§€ {page}ì—ì„œ {page_count}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ (ì´ {collected_count}ê°œ)")
            
            # ë‚ ì§œ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ë©´ ì¤‘ë‹¨
            if stop_collecting:
                print(f"  âœ“ ëª©í‘œ ë‚ ì§œ ë²”ìœ„ ë„ë‹¬. ìˆ˜ì§‘ ì¤‘ë‹¨.")
                break
        
        print(f"\nâœ“ ì´ {len(self.news_data)}ê°œì˜ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
        
        if self.news_data:
            df = pd.DataFrame(self.news_data)
            # ì‹œê°„ìˆœ ì •ë ¬
            df = df.sort_values('timestamp', ascending=True)
            return df
        else:
            return pd.DataFrame()


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("=" * 60)
    print("ì½”ì¸ë‹ˆìŠ¤ ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘")
    print("=" * 60)
    
    # ìˆ˜ì§‘ ì„¤ì •
    start_date = datetime(2025, 1, 1)
    max_pages = 100  # ìµœëŒ€ 100í˜ì´ì§€
    output_file = 'data/coinness_data.csv'
    
    # ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”
    collector = CoinnessCollector()
    
    # ë‰´ìŠ¤ ìˆ˜ì§‘
    df = collector.collect_news(max_pages=max_pages, start_date=start_date)
    
    # ì €ì¥
    if not df.empty:
        # ë°ì´í„° ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs('data', exist_ok=True)
        
        # CSV ì €ì¥
        df.to_csv(output_file, index=False, encoding='utf-8-sig')
        print(f"\nâœ… ë°ì´í„° ì €ì¥ ì™„ë£Œ: {output_file}")
        print(f"   ì´ {len(df)}ê°œ ë‰´ìŠ¤ ê¸°ì‚¬")
        print(f"   ê¸°ê°„: {df['timestamp'].min()} ~ {df['timestamp'].max()}")
        
        # í†µê³„ ì¶œë ¥
        print(f"\nğŸ“Š ìˆ˜ì§‘ í†µê³„:")
        print(f"   í‰ê·  ê°ì • ì ìˆ˜: {df['sentiment_compound'].mean():.3f}")
        print(f"   ê¸ì • ë¹„ìœ¨: {(df['sentiment_compound'] > 0.05).sum() / len(df) * 100:.1f}%")
        print(f"   ë¶€ì • ë¹„ìœ¨: {(df['sentiment_compound'] < -0.05).sum() / len(df) * 100:.1f}%")
        print(f"   ì¤‘ë¦½ ë¹„ìœ¨: {((df['sentiment_compound'] >= -0.05) & (df['sentiment_compound'] <= 0.05)).sum() / len(df) * 100:.1f}%")
    else:
        print(f"\nâš ï¸  ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        print(f"   ì½”ì¸ë‹ˆìŠ¤ ì›¹ì‚¬ì´íŠ¸ êµ¬ì¡°ê°€ ë³€ê²½ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        print(f"   HTML êµ¬ì¡°ë¥¼ í™•ì¸í•˜ê³  ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì—…ë°ì´íŠ¸í•´ì£¼ì„¸ìš”.")


if __name__ == '__main__':
    main()

