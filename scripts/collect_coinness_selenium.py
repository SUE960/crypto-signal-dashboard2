"""
ì½”ì¸ë‹ˆìŠ¤(Coinness) ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸ (Selenium ë²„ì „)

JavaScriptë¡œ ë Œë”ë§ë˜ëŠ” React ê¸°ë°˜ ì‚¬ì´íŠ¸ë¥¼ Seleniumìœ¼ë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
"""

import os
import time
import random
from datetime import datetime, timedelta
import pandas as pd
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
import re

# ê°ì • ë¶„ì„ê¸° ì´ˆê¸°í™”
sentiment_analyzer = SentimentIntensityAnalyzer()


class CoinnessSeleniumCollector:
    """Seleniumì„ ì‚¬ìš©í•œ ì½”ì¸ë‹ˆìŠ¤ ë‰´ìŠ¤ ìˆ˜ì§‘ê¸°"""
    
    def __init__(self, headless=True):
        """
        ì´ˆê¸°í™”
        
        Args:
            headless: ë¸Œë¼ìš°ì €ë¥¼ ìˆ¨ê¹€ ëª¨ë“œë¡œ ì‹¤í–‰í• ì§€ ì—¬ë¶€
        """
        self.base_url = 'https://coinness.com'
        self.driver = None
        self.headless = headless
        self.news_data = []
        
    def setup_driver(self):
        """Chrome WebDriver ì„¤ì • (Selenium Manager ì‚¬ìš©)"""
        print("Chrome WebDriver ì„¤ì • ì¤‘...")
        print("  (Selenium Managerê°€ ìë™ìœ¼ë¡œ ChromeDriverë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤)")
        
        chrome_options = Options()
        
        # Headless ëª¨ë“œ ì„¤ì •
        if self.headless:
            chrome_options.add_argument('--headless=new')
            print("  ëª¨ë“œ: Headless (ë°±ê·¸ë¼ìš´ë“œ)")
        else:
            print("  ëª¨ë“œ: GUI (ë¸Œë¼ìš°ì € í‘œì‹œ)")
        
        # í•„ìˆ˜ ì˜µì…˜
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        
        # ìë™í™” ê°ì§€ íšŒí”¼
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        # User-Agent
        chrome_options.add_argument(
            'user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) '
            'AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36'
        )
        
        # Chrome ë°”ì´ë„ˆë¦¬ ê²½ë¡œ ëª…ì‹œ (macOS)
        chrome_options.binary_location = '/Applications/Google Chrome.app/Contents/MacOS/Google Chrome'
        
        try:
            # Selenium 4.6+ ì˜ Selenium Managerê°€ ìë™ìœ¼ë¡œ ChromeDriver ê´€ë¦¬
            self.driver = webdriver.Chrome(options=chrome_options)
            print("âœ“ Chrome WebDriver ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            print(f"âœ— Chrome WebDriver ì„¤ì • ì‹¤íŒ¨: {e}")
            print("\ní•´ê²° ë°©ë²•:")
            print("1. Chrome ë¸Œë¼ìš°ì €ê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸")
            print("2. í„°ë¯¸ë„ì—ì„œ ë‹¤ìŒ ëª…ë ¹ì–´ ì‹¤í–‰:")
            print("   brew install --cask google-chrome")
            raise
        
        # íƒ€ì„ì•„ì›ƒ ì„¤ì •
        self.driver.implicitly_wait(10)
    
    def close_driver(self):
        """WebDriver ì¢…ë£Œ"""
        if self.driver:
            self.driver.quit()
            print("âœ“ Chrome WebDriver ì¢…ë£Œ")
    
    def scroll_page(self, scroll_pause_time=2):
        """
        í˜ì´ì§€ë¥¼ ìŠ¤í¬ë¡¤í•˜ì—¬ ë™ì  ì½˜í…ì¸  ë¡œë”©
        
        Args:
            scroll_pause_time: ìŠ¤í¬ë¡¤ í›„ ëŒ€ê¸° ì‹œê°„
        """
        # í˜ì´ì§€ ëê¹Œì§€ ìŠ¤í¬ë¡¤
        last_height = self.driver.execute_script("return document.body.scrollHeight")
        
        for _ in range(3):  # ìµœëŒ€ 3ë²ˆ ìŠ¤í¬ë¡¤
            # ìŠ¤í¬ë¡¤ ë‹¤ìš´
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            
            # ë¡œë”© ëŒ€ê¸°
            time.sleep(scroll_pause_time)
            
            # ìƒˆ ë†’ì´ ê³„ì‚°
            new_height = self.driver.execute_script("return document.body.scrollHeight")
            
            # ë” ì´ìƒ ìŠ¤í¬ë¡¤í•  ìˆ˜ ì—†ìœ¼ë©´ ì¤‘ë‹¨
            if new_height == last_height:
                break
            
            last_height = new_height
    
    def parse_time_with_date(self, time_str, date_str):
        """
        ì‹œê°„ê³¼ ë‚ ì§œ ë¬¸ìì—´ì„ datetimeìœ¼ë¡œ ë³€í™˜
        
        Args:
            time_str: ì‹œê°„ ë¬¸ìì—´ (ì˜ˆ: "13:30")
            date_str: ë‚ ì§œ ë¬¸ìì—´ (ì˜ˆ: "2025ë…„ 11ì›” 30ì¼ ì¼ìš”ì¼")
            
        Returns:
            datetime: íŒŒì‹±ëœ ì‹œê°„
        """
        now = datetime.now()
        
        try:
            # ë‚ ì§œ íŒŒì‹±
            if 'ë…„' in date_str and 'ì›”' in date_str and 'ì¼' in date_str:
                numbers = re.findall(r'\d+', date_str)
                
                if len(numbers) >= 3:
                    year = int(numbers[0])
                    month = int(numbers[1])
                    day = int(numbers[2])
                    
                    # ì‹œê°„ íŒŒì‹±
                    if ':' in time_str:
                        time_parts = time_str.split(':')
                        hour = int(time_parts[0])
                        minute = int(time_parts[1]) if len(time_parts) > 1 else 0
                    else:
                        hour = 0
                        minute = 0
                    
                    return datetime(year, month, day, hour, minute)
            
            return now
                
        except Exception as e:
            print(f"  âš ï¸  ì‹œê°„ íŒŒì‹± ì˜¤ë¥˜: time={time_str}, date={date_str} - {e}")
            return now
    
    def parse_news_articles(self, html_content):
        """
        HTMLì—ì„œ ë‰´ìŠ¤ ê¸°ì‚¬ íŒŒì‹±
        
        Args:
            html_content: HTML ë¬¸ìì—´
            
        Returns:
            list: íŒŒì‹±ëœ ë‰´ìŠ¤ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        soup = BeautifulSoup(html_content, 'html.parser')
        articles_data = []
        
        # ArticleWrapper í´ë˜ìŠ¤ë¥¼ ê°€ì§„ a íƒœê·¸ ì°¾ê¸°
        articles = soup.find_all('a', class_=lambda x: x and 'ArticleWrapper' in x)
        
        for article in articles:
            try:
                # ë§í¬
                link = article.get('href', '')
                
                # ì œëª©
                title_elem = article.find('h3', class_=lambda x: x and 'ArticleTitle' in x)
                if not title_elem:
                    continue
                title = title_elem.get_text(strip=True)
                
                # ì‹œê°„ ì •ë³´
                time_wrap = article.find('div', class_=lambda x: x and 'TimeWrap' in x)
                if time_wrap:
                    time_badge = time_wrap.find('span', class_='time-badge')
                    time_str = time_badge.get_text(strip=True) if time_badge else ''
                    date_text = time_wrap.get_text(strip=True).replace(time_str, '').strip()
                else:
                    time_str = ''
                    date_text = ''
                
                # ì‹œê°„ íŒŒì‹±
                pub_time = self.parse_time_with_date(time_str, date_text)
                
                # ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°
                content_elem = article.find('p', class_=lambda x: x and 'ArticleDesc' in x)
                content = content_elem.get_text(strip=True) if content_elem else ''
                
                # ê°ì • ë¶„ì„
                text_for_sentiment = f"{title} {content}"
                sentiment = sentiment_analyzer.polarity_scores(text_for_sentiment)
                
                articles_data.append({
                    'timestamp': pub_time,
                    'title': title,
                    'content': content,
                    'link': link,
                    'sentiment_compound': sentiment['compound'],
                    'sentiment_positive': sentiment['pos'],
                    'sentiment_negative': sentiment['neg'],
                    'sentiment_neutral': sentiment['neu'],
                })
                
            except Exception as e:
                print(f"  âš ï¸  ê¸°ì‚¬ íŒŒì‹± ì˜¤ë¥˜: {e}")
                continue
        
        return articles_data
    
    def collect_news(self, max_pages=50, start_date=None):
        """
        ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
        
        Args:
            max_pages: ìˆ˜ì§‘í•  ìµœëŒ€ í˜ì´ì§€ ìˆ˜
            start_date: ìˆ˜ì§‘ ì‹œì‘ ë‚ ì§œ
            
        Returns:
            DataFrame: ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ë°ì´í„°
        """
        if start_date is None:
            start_date = datetime(2025, 1, 1)
        
        print(f"\nì½”ì¸ë‹ˆìŠ¤ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘ (Selenium)...")
        print(f"  ìˆ˜ì§‘ ê¸°ê°„: {start_date.date()} ~ í˜„ì¬")
        print(f"  ìµœëŒ€ í˜ì´ì§€: {max_pages}")
        
        # WebDriver ì„¤ì •
        self.setup_driver()
        
        try:
            collected_count = 0
            
            for page in range(1, max_pages + 1):
                print(f"\ní˜ì´ì§€ {page}/{max_pages} ìˆ˜ì§‘ ì¤‘...")
                
                # í˜ì´ì§€ URL
                url = f"{self.base_url}/article?page={page}"
                
                try:
                    # í˜ì´ì§€ ë¡œë“œ
                    self.driver.get(url)
                    
                    # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° (ArticleWrapperê°€ ë‚˜íƒ€ë‚  ë•Œê¹Œì§€)
                    wait = WebDriverWait(self.driver, 15)
                    wait.until(
                        EC.presence_of_element_located((By.CLASS_NAME, "ArticleWrapper-sc-42qvi5-0"))
                    )
                    
                    # ìŠ¤í¬ë¡¤í•˜ì—¬ ë™ì  ì½˜í…ì¸  ë¡œë”©
                    self.scroll_page(scroll_pause_time=1.5)
                    
                    # ëœë¤ ì§€ì—° (1~3ì´ˆ)
                    time.sleep(random.uniform(1, 3))
                    
                    # HTML ê°€ì ¸ì˜¤ê¸°
                    html_content = self.driver.page_source
                    
                    # ê¸°ì‚¬ íŒŒì‹±
                    articles = self.parse_news_articles(html_content)
                    
                    if not articles:
                        print(f"  âš ï¸  í˜ì´ì§€ {page}ì—ì„œ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        if page == 1:
                            print(f"  ğŸ’¡ ì²« í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                            break
                        continue
                    
                    # ë‚ ì§œ í•„í„°ë§ ë° ì €ì¥
                    page_count = 0
                    stop_collecting = False
                    
                    for article in articles:
                        if article['timestamp'] < start_date:
                            stop_collecting = True
                            break
                        
                        self.news_data.append(article)
                        page_count += 1
                        collected_count += 1
                    
                    print(f"  âœ“ í˜ì´ì§€ {page}ì—ì„œ {page_count}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ (ì´ {collected_count}ê°œ)")
                    
                    # ë‚ ì§œ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ë©´ ì¤‘ë‹¨
                    if stop_collecting:
                        print(f"  âœ“ ëª©í‘œ ë‚ ì§œ ë²”ìœ„ ë„ë‹¬. ìˆ˜ì§‘ ì¤‘ë‹¨.")
                        break
                    
                except Exception as e:
                    print(f"  âœ— í˜ì´ì§€ {page} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    continue
            
            print(f"\nâœ“ ì´ {len(self.news_data)}ê°œì˜ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
            
            if self.news_data:
                df = pd.DataFrame(self.news_data)
                df = df.sort_values('timestamp', ascending=True)
                return df
            else:
                return pd.DataFrame()
                
        finally:
            # WebDriver ì¢…ë£Œ
            self.close_driver()


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("=" * 60)
    print("ì½”ì¸ë‹ˆìŠ¤ ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ (Selenium)")
    print("=" * 60)
    
    # ìˆ˜ì§‘ ì„¤ì •
    start_date = datetime(2025, 1, 1)
    max_pages = 100
    output_file = 'data/coinness_data.csv'
    
    # ìˆ˜ì§‘ê¸° ì´ˆê¸°í™” (headless=True: ë¸Œë¼ìš°ì € ìˆ¨ê¹€)
    collector = CoinnessSeleniumCollector(headless=True)
    
    # ë‰´ìŠ¤ ìˆ˜ì§‘
    df = collector.collect_news(max_pages=max_pages, start_date=start_date)
    
    # ì €ì¥
    if not df.empty:
        # ë°ì´í„° ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs('data', exist_ok=True)
        
        # CSV ì €ì¥
        df.to_csv(output_file, index=False, encoding='utf-8-sig')
        print(f"\nâœ… ë°ì´í„° ì €ì¥ ì™„ë£Œ: {output_file}")
        print(f"   ì´ {len(df)}ê°œ ë‰´ìŠ¤ ê¸°ì‚¬")
        print(f"   ê¸°ê°„: {df['timestamp'].min()} ~ {df['timestamp'].max()}")
        
        # í†µê³„ ì¶œë ¥
        print(f"\nğŸ“Š ìˆ˜ì§‘ í†µê³„:")
        print(f"   í‰ê·  ê°ì • ì ìˆ˜: {df['sentiment_compound'].mean():.3f}")
        print(f"   ê¸ì • ë¹„ìœ¨: {(df['sentiment_compound'] > 0.05).sum() / len(df) * 100:.1f}%")
        print(f"   ë¶€ì • ë¹„ìœ¨: {(df['sentiment_compound'] < -0.05).sum() / len(df) * 100:.1f}%")
        print(f"   ì¤‘ë¦½ ë¹„ìœ¨: {((df['sentiment_compound'] >= -0.05) & (df['sentiment_compound'] <= 0.05)).sum() / len(df) * 100:.1f}%")
    else:
        print(f"\nâš ï¸  ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")


if __name__ == '__main__':
    main()

