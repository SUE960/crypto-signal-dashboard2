"""
í…”ë ˆê·¸ë¨ ì»¤ë®¤ë‹ˆí‹° í™œë™ ìƒê´€ê´€ê³„ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸

í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ìˆ˜ì™€ ê³ ë˜ ê±°ë˜ëŸ‰, ê°€ê²© ë³€í™”ì˜ ìƒê´€ê´€ê³„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
"""

import pandas as pd
import numpy as np
from scipy import stats
from statsmodels.tsa.stattools import grangercausalitytests
import warnings
import sys
import os

warnings.filterwarnings('ignore')

# ê²½ë¡œ ì¶”ê°€
sys.path.append('/Volumes/T7/class/2025-FALL/big_data')

def load_data():
    """ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ"""
    try:
        df = pd.read_csv('/Volumes/T7/class/2025-FALL/big_data/data/processed_data.csv')
        df['timestamp'] = pd.to_datetime(df['timestamp'])
        return df
    except FileNotFoundError:
        print("âŒ ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("ë¨¼ì € 'python scripts/preprocess_data.py'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        sys.exit(1)


def analyze_basic_correlation(df):
    """ê¸°ë³¸ ìƒê´€ê´€ê³„ ë¶„ì„"""
    print("\n" + "=" * 80)
    print("1. í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜ (Pearson Correlation) - ì„ í˜• ê´€ê³„")
    print("=" * 80)
    
    # ê²°ì¸¡ì¹˜ ì œê±°
    df_clean = df[['message_count', 'ETH_close', 'BTC_close', 'tx_frequency', 'tx_amount', 'avg_sentiment']].dropna()
    
    if len(df_clean) < 10:
        print("âš ï¸ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤ (ìµœì†Œ 10ê°œ í•„ìš”)")
        return
    
    results = []
    
    # 1. í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ìˆ˜ vs ETH ê°€ê²©
    corr, p_val = stats.pearsonr(df_clean['message_count'], df_clean['ETH_close'])
    results.append(('ë©”ì‹œì§€ ìˆ˜ â†” ETH ê°€ê²©', corr, p_val))
    print(f"\nğŸ“Š í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ìˆ˜ â†” ETH ê°€ê²©")
    print(f"   ìƒê´€ê³„ìˆ˜: {corr:+.4f}")
    print(f"   P-value: {p_val:.6f} {'âœ… ìœ ì˜í•¨ (p<0.05)' if p_val < 0.05 else 'âŒ ìœ ì˜í•˜ì§€ ì•ŠìŒ'}")
    print(f"   í•´ì„: {'ì–‘ì˜ ìƒê´€ê´€ê³„' if corr > 0 else 'ìŒì˜ ìƒê´€ê´€ê³„'} - " + 
          f"{'ê°•í•¨' if abs(corr) > 0.7 else 'ì¤‘ê°„' if abs(corr) > 0.4 else 'ì•½í•¨'} (|r|={abs(corr):.2f})")
    
    # 2. í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ìˆ˜ vs BTC ê°€ê²©
    corr, p_val = stats.pearsonr(df_clean['message_count'], df_clean['BTC_close'])
    results.append(('ë©”ì‹œì§€ ìˆ˜ â†” BTC ê°€ê²©', corr, p_val))
    print(f"\nğŸ“Š í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ìˆ˜ â†” BTC ê°€ê²©")
    print(f"   ìƒê´€ê³„ìˆ˜: {corr:+.4f}")
    print(f"   P-value: {p_val:.6f} {'âœ… ìœ ì˜í•¨ (p<0.05)' if p_val < 0.05 else 'âŒ ìœ ì˜í•˜ì§€ ì•ŠìŒ'}")
    print(f"   í•´ì„: {'ì–‘ì˜ ìƒê´€ê´€ê³„' if corr > 0 else 'ìŒì˜ ìƒê´€ê´€ê³„'} - " + 
          f"{'ê°•í•¨' if abs(corr) > 0.7 else 'ì¤‘ê°„' if abs(corr) > 0.4 else 'ì•½í•¨'} (|r|={abs(corr):.2f})")
    
    # 3. í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ìˆ˜ vs ê³ ë˜ ê±°ë˜ ë¹ˆë„
    corr, p_val = stats.pearsonr(df_clean['message_count'], df_clean['tx_frequency'])
    results.append(('ë©”ì‹œì§€ ìˆ˜ â†” ê³ ë˜ ê±°ë˜ ë¹ˆë„', corr, p_val))
    print(f"\nğŸ“Š í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ìˆ˜ â†” ê³ ë˜ ê±°ë˜ ë¹ˆë„")
    print(f"   ìƒê´€ê³„ìˆ˜: {corr:+.4f}")
    print(f"   P-value: {p_val:.6f} {'âœ… ìœ ì˜í•¨ (p<0.05)' if p_val < 0.05 else 'âŒ ìœ ì˜í•˜ì§€ ì•ŠìŒ'}")
    print(f"   í•´ì„: {'ì–‘ì˜ ìƒê´€ê´€ê³„' if corr > 0 else 'ìŒì˜ ìƒê´€ê´€ê³„'} - " + 
          f"{'ê°•í•¨' if abs(corr) > 0.7 else 'ì¤‘ê°„' if abs(corr) > 0.4 else 'ì•½í•¨'} (|r|={abs(corr):.2f})")
    
    # 4. í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ìˆ˜ vs ê³ ë˜ ê±°ë˜ ê¸ˆì•¡
    corr, p_val = stats.pearsonr(df_clean['message_count'], df_clean['tx_amount'])
    results.append(('ë©”ì‹œì§€ ìˆ˜ â†” ê³ ë˜ ê±°ë˜ ê¸ˆì•¡', corr, p_val))
    print(f"\nğŸ“Š í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ìˆ˜ â†” ê³ ë˜ ê±°ë˜ ê¸ˆì•¡")
    print(f"   ìƒê´€ê³„ìˆ˜: {corr:+.4f}")
    print(f"   P-value: {p_val:.6f} {'âœ… ìœ ì˜í•¨ (p<0.05)' if p_val < 0.05 else 'âŒ ìœ ì˜í•˜ì§€ ì•ŠìŒ'}")
    print(f"   í•´ì„: {'ì–‘ì˜ ìƒê´€ê´€ê³„' if corr > 0 else 'ìŒì˜ ìƒê´€ê´€ê³„'} - " + 
          f"{'ê°•í•¨' if abs(corr) > 0.7 else 'ì¤‘ê°„' if abs(corr) > 0.4 else 'ì•½í•¨'} (|r|={abs(corr):.2f})")
    
    # 5. í…”ë ˆê·¸ë¨ ê°ì • vs ETH ê°€ê²©
    corr, p_val = stats.pearsonr(df_clean['avg_sentiment'], df_clean['ETH_close'])
    results.append(('ê°ì • ì ìˆ˜ â†” ETH ê°€ê²©', corr, p_val))
    print(f"\nğŸ“Š í…”ë ˆê·¸ë¨ ê°ì • ì ìˆ˜ â†” ETH ê°€ê²©")
    print(f"   ìƒê´€ê³„ìˆ˜: {corr:+.4f}")
    print(f"   P-value: {p_val:.6f} {'âœ… ìœ ì˜í•¨ (p<0.05)' if p_val < 0.05 else 'âŒ ìœ ì˜í•˜ì§€ ì•ŠìŒ'}")
    print(f"   í•´ì„: {'ì–‘ì˜ ìƒê´€ê´€ê³„' if corr > 0 else 'ìŒì˜ ìƒê´€ê´€ê³„'} - " + 
          f"{'ê°•í•¨' if abs(corr) > 0.7 else 'ì¤‘ê°„' if abs(corr) > 0.4 else 'ì•½í•¨'} (|r|={abs(corr):.2f})")
    
    return results


def analyze_lag_correlation(df):
    """ì‹œì°¨ ìƒê´€ê´€ê³„ ë¶„ì„"""
    print("\n" + "=" * 80)
    print("2. ì‹œì°¨ ìƒê´€ê´€ê³„ (Lag Correlation) - í…”ë ˆê·¸ë¨ì´ ì„ í–‰í•˜ëŠ”ì§€ í™•ì¸")
    print("=" * 80)
    
    df_clean = df[['message_count', 'ETH_close', 'tx_frequency']].dropna()
    
    if len(df_clean) < 50:
        print("âš ï¸ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤ (ìµœì†Œ 50ê°œ í•„ìš”)")
        return
    
    max_lag = min(12, len(df_clean) // 10)  # ìµœëŒ€ 12ì‹œê°„ ë˜ëŠ” ë°ì´í„°ì˜ 10%
    
    print(f"\nğŸ” í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ìˆ˜ â†’ ETH ê°€ê²© (ìµœëŒ€ {max_lag}ì‹œê°„ ì‹œì°¨)")
    print("   (í…”ë ˆê·¸ë¨ í™œë™ í›„ ëª‡ ì‹œê°„ ë’¤ì— ê°€ê²©ì´ ë³€í•˜ëŠ”ê°€?)")
    
    lag_results_eth = []
    for lag in range(0, max_lag + 1):
        if len(df_clean) > lag:
            x = df_clean['message_count'].iloc[:-lag] if lag > 0 else df_clean['message_count']
            y = df_clean['ETH_close'].iloc[lag:] if lag > 0 else df_clean['ETH_close']
            
            if len(x) > 0 and len(y) > 0 and len(x) == len(y):
                corr, p_val = stats.pearsonr(x, y)
                lag_results_eth.append({
                    'lag': lag,
                    'correlation': corr,
                    'p_value': p_val,
                    'significant': p_val < 0.05
                })
                sig_marker = 'âœ…' if p_val < 0.05 else '  '
                print(f"   Lag {lag:2d}ì‹œê°„: r={corr:+.4f}, p={p_val:.4f} {sig_marker}")
    
    if lag_results_eth:
        max_corr_eth = max(lag_results_eth, key=lambda x: abs(x['correlation']))
        print(f"\n   ğŸ’¡ ìµœëŒ€ ìƒê´€: Lag {max_corr_eth['lag']}ì‹œê°„, r={max_corr_eth['correlation']:+.4f}")
        if max_corr_eth['significant']:
            print(f"   âœ… í…”ë ˆê·¸ë¨ í™œë™ í›„ {max_corr_eth['lag']}ì‹œê°„ ë’¤ ETH ê°€ê²©ê³¼ ìœ ì˜ë¯¸í•œ ìƒê´€!")
    
    print(f"\nğŸ” í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ìˆ˜ â†’ ê³ ë˜ ê±°ë˜ ë¹ˆë„ (ìµœëŒ€ {max_lag}ì‹œê°„ ì‹œì°¨)")
    print("   (í…”ë ˆê·¸ë¨ í™œë™ í›„ ëª‡ ì‹œê°„ ë’¤ì— ê³ ë˜ê°€ ì›€ì§ì´ëŠ”ê°€?)")
    
    lag_results_whale = []
    for lag in range(0, max_lag + 1):
        if len(df_clean) > lag:
            x = df_clean['message_count'].iloc[:-lag] if lag > 0 else df_clean['message_count']
            y = df_clean['tx_frequency'].iloc[lag:] if lag > 0 else df_clean['tx_frequency']
            
            if len(x) > 0 and len(y) > 0 and len(x) == len(y):
                corr, p_val = stats.pearsonr(x, y)
                lag_results_whale.append({
                    'lag': lag,
                    'correlation': corr,
                    'p_value': p_val,
                    'significant': p_val < 0.05
                })
                sig_marker = 'âœ…' if p_val < 0.05 else '  '
                print(f"   Lag {lag:2d}ì‹œê°„: r={corr:+.4f}, p={p_val:.4f} {sig_marker}")
    
    if lag_results_whale:
        max_corr_whale = max(lag_results_whale, key=lambda x: abs(x['correlation']))
        print(f"\n   ğŸ’¡ ìµœëŒ€ ìƒê´€: Lag {max_corr_whale['lag']}ì‹œê°„, r={max_corr_whale['correlation']:+.4f}")
        if max_corr_whale['significant']:
            print(f"   âœ… í…”ë ˆê·¸ë¨ í™œë™ í›„ {max_corr_whale['lag']}ì‹œê°„ ë’¤ ê³ ë˜ ê±°ë˜ì™€ ìœ ì˜ë¯¸í•œ ìƒê´€!")
    
    return lag_results_eth, lag_results_whale


def analyze_change_correlation(df):
    """ë³€í™”ìœ¨ ìƒê´€ê´€ê³„ ë¶„ì„"""
    print("\n" + "=" * 80)
    print("3. ë³€í™”ìœ¨ ìƒê´€ê´€ê³„ - ê¸‰ë³€ ì‹œ ë™ì‹œ ì›€ì§ì„")
    print("=" * 80)
    
    # ë³€í™”ìœ¨ ê³„ì‚°
    df_changes = df.copy()
    df_changes['msg_change'] = df_changes['message_count'].pct_change() * 100
    df_changes['eth_change'] = df_changes['ETH_close'].pct_change() * 100
    df_changes['whale_freq_change'] = df_changes['tx_frequency'].pct_change() * 100
    df_changes['whale_amt_change'] = df_changes['tx_amount'].pct_change() * 100
    
    # ë¬´í•œëŒ€ ì œê±°
    df_changes = df_changes.replace([np.inf, -np.inf], np.nan)
    
    # 1. ë©”ì‹œì§€ ë³€í™”ìœ¨ vs ETH ê°€ê²© ë³€í™”ìœ¨
    df_clean = df_changes[['msg_change', 'eth_change']].dropna()
    if len(df_clean) > 10:
        corr, p_val = stats.pearsonr(df_clean['msg_change'], df_clean['eth_change'])
        print(f"\nğŸ“Š ë©”ì‹œì§€ ë³€í™”ìœ¨ â†” ETH ê°€ê²© ë³€í™”ìœ¨")
        print(f"   ìƒê´€ê³„ìˆ˜: {corr:+.4f}")
        print(f"   P-value: {p_val:.6f} {'âœ… ìœ ì˜í•¨ (p<0.05)' if p_val < 0.05 else 'âŒ ìœ ì˜í•˜ì§€ ì•ŠìŒ'}")
        if p_val < 0.05:
            print(f"   âœ… í…”ë ˆê·¸ë¨ í™œë™ì´ ê¸‰ì¦/ê¸‰ê°í•  ë•Œ ETH ê°€ê²©ë„ í•¨ê»˜ ì›€ì§ì„!")
    
    # 2. ë©”ì‹œì§€ ë³€í™”ìœ¨ vs ê³ ë˜ ê±°ë˜ ë¹ˆë„ ë³€í™”ìœ¨
    df_clean = df_changes[['msg_change', 'whale_freq_change']].dropna()
    if len(df_clean) > 10:
        corr, p_val = stats.pearsonr(df_clean['msg_change'], df_clean['whale_freq_change'])
        print(f"\nğŸ“Š ë©”ì‹œì§€ ë³€í™”ìœ¨ â†” ê³ ë˜ ê±°ë˜ ë¹ˆë„ ë³€í™”ìœ¨")
        print(f"   ìƒê´€ê³„ìˆ˜: {corr:+.4f}")
        print(f"   P-value: {p_val:.6f} {'âœ… ìœ ì˜í•¨ (p<0.05)' if p_val < 0.05 else 'âŒ ìœ ì˜í•˜ì§€ ì•ŠìŒ'}")
        if p_val < 0.05:
            print(f"   âœ… í…”ë ˆê·¸ë¨ í™œë™ì´ ê¸‰ì¦/ê¸‰ê°í•  ë•Œ ê³ ë˜ ê±°ë˜ë„ í•¨ê»˜ ì›€ì§ì„!")
    
    # 3. ë©”ì‹œì§€ ë³€í™”ìœ¨ vs ê³ ë˜ ê±°ë˜ ê¸ˆì•¡ ë³€í™”ìœ¨
    df_clean = df_changes[['msg_change', 'whale_amt_change']].dropna()
    if len(df_clean) > 10:
        corr, p_val = stats.pearsonr(df_clean['msg_change'], df_clean['whale_amt_change'])
        print(f"\nğŸ“Š ë©”ì‹œì§€ ë³€í™”ìœ¨ â†” ê³ ë˜ ê±°ë˜ ê¸ˆì•¡ ë³€í™”ìœ¨")
        print(f"   ìƒê´€ê³„ìˆ˜: {corr:+.4f}")
        print(f"   P-value: {p_val:.6f} {'âœ… ìœ ì˜í•¨ (p<0.05)' if p_val < 0.05 else 'âŒ ìœ ì˜í•˜ì§€ ì•ŠìŒ'}")
        if p_val < 0.05:
            print(f"   âœ… í…”ë ˆê·¸ë¨ í™œë™ì´ ê¸‰ì¦/ê¸‰ê°í•  ë•Œ ê³ ë˜ ê±°ë˜ ê¸ˆì•¡ë„ í•¨ê»˜ ì›€ì§ì„!")


def analyze_granger_causality(df):
    """ê·¸ëœì € ì¸ê³¼ê´€ê³„ ê²€ì •"""
    print("\n" + "=" * 80)
    print("4. ê·¸ëœì € ì¸ê³¼ê´€ê³„ (Granger Causality) - í…”ë ˆê·¸ë¨ì´ ì›ì¸ì¸ê°€?")
    print("=" * 80)
    print("   (H0: í…”ë ˆê·¸ë¨ í™œë™ì€ ê°€ê²©/ê±°ë˜ì˜ ì›ì¸ì´ ì•„ë‹ˆë‹¤)")
    
    df_clean = df[['message_count', 'ETH_close', 'tx_frequency']].dropna()
    
    if len(df_clean) < 50:
        print("âš ï¸ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤ (ìµœì†Œ 50ê°œ í•„ìš”)")
        return
    
    max_lag = min(8, len(df_clean) // 20)
    
    try:
        # 1. í…”ë ˆê·¸ë¨ â†’ ETH ê°€ê²©
        print(f"\nğŸ” í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ìˆ˜ â†’ ETH ê°€ê²© (ìµœëŒ€ {max_lag}ì‹œê°„ lag)")
        test_data = df_clean[['ETH_close', 'message_count']].copy()
        test_result = grangercausalitytests(test_data, max_lag, verbose=False)
        
        significant_lags = []
        for lag in range(1, max_lag + 1):
            p_value = test_result[lag][0]['ssr_ftest'][1]
            if p_value < 0.05:
                significant_lags.append(lag)
                print(f"   Lag {lag}: p={p_value:.4f} âœ… ìœ ì˜í•¨!")
            else:
                print(f"   Lag {lag}: p={p_value:.4f}")
        
        if significant_lags:
            print(f"\n   âœ… í…”ë ˆê·¸ë¨ í™œë™ì´ ETH ê°€ê²©ì— ì˜í–¥ì„ ì¤Œ! (Lag: {significant_lags})")
        else:
            print(f"\n   âŒ í…”ë ˆê·¸ë¨ í™œë™ì´ ETH ê°€ê²©ì— ìœ ì˜ë¯¸í•œ ì˜í–¥ì„ ì£¼ì§€ ì•ŠìŒ")
        
    except Exception as e:
        print(f"   âš ï¸ ê·¸ëœì € ì¸ê³¼ê´€ê³„ ê²€ì • ì‹¤íŒ¨: {e}")
    
    try:
        # 2. í…”ë ˆê·¸ë¨ â†’ ê³ ë˜ ê±°ë˜
        print(f"\nğŸ” í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ìˆ˜ â†’ ê³ ë˜ ê±°ë˜ ë¹ˆë„ (ìµœëŒ€ {max_lag}ì‹œê°„ lag)")
        test_data = df_clean[['tx_frequency', 'message_count']].copy()
        test_result = grangercausalitytests(test_data, max_lag, verbose=False)
        
        significant_lags = []
        for lag in range(1, max_lag + 1):
            p_value = test_result[lag][0]['ssr_ftest'][1]
            if p_value < 0.05:
                significant_lags.append(lag)
                print(f"   Lag {lag}: p={p_value:.4f} âœ… ìœ ì˜í•¨!")
            else:
                print(f"   Lag {lag}: p={p_value:.4f}")
        
        if significant_lags:
            print(f"\n   âœ… í…”ë ˆê·¸ë¨ í™œë™ì´ ê³ ë˜ ê±°ë˜ì— ì˜í–¥ì„ ì¤Œ! (Lag: {significant_lags})")
        else:
            print(f"\n   âŒ í…”ë ˆê·¸ë¨ í™œë™ì´ ê³ ë˜ ê±°ë˜ì— ìœ ì˜ë¯¸í•œ ì˜í–¥ì„ ì£¼ì§€ ì•ŠìŒ")
        
    except Exception as e:
        print(f"   âš ï¸ ê·¸ëœì € ì¸ê³¼ê´€ê³„ ê²€ì • ì‹¤íŒ¨: {e}")


def generate_summary(df, basic_results):
    """ì¢…í•© ìš”ì•½"""
    print("\n" + "=" * 80)
    print("ğŸ“Š ì¢…í•© ë¶„ì„ ê²°ê³¼")
    print("=" * 80)
    
    # ë°ì´í„° ê¸°ë³¸ ì •ë³´
    print(f"\nğŸ“… ë¶„ì„ ê¸°ê°„: {df['timestamp'].min().date()} ~ {df['timestamp'].max().date()}")
    print(f"ğŸ“Š ì´ ë°ì´í„°: {len(df)} ì‹œê°„")
    print(f"ğŸ’¬ í…”ë ˆê·¸ë¨ ì´ ë©”ì‹œì§€: {df['message_count'].sum():.0f}ê°œ")
    print(f"ğŸ’¬ í‰ê·  ì‹œê°„ë‹¹ ë©”ì‹œì§€: {df['message_count'].mean():.2f}ê°œ")
    print(f"ğŸ‹ ê³ ë˜ ê±°ë˜ ì´ ê±´ìˆ˜: {df['tx_frequency'].sum():.0f}ê±´")
    
    # ìœ ì˜ë¯¸í•œ ìƒê´€ê´€ê³„ ì¹´ìš´íŠ¸
    if basic_results:
        significant_count = sum([1 for _, _, p in basic_results if p < 0.05])
        total_count = len(basic_results)
        
        print(f"\nğŸ¯ ìœ ì˜ë¯¸í•œ ìƒê´€ê´€ê³„: {significant_count}/{total_count}ê°œ")
        
        if significant_count >= 2:
            print("\nâœ… ê²°ë¡ : í…”ë ˆê·¸ë¨ í™œë™ê³¼ ì‹œì¥ ì§€í‘œ ê°„ ìœ ì˜ë¯¸í•œ ìƒê´€ê´€ê³„ê°€ ì¡´ì¬í•©ë‹ˆë‹¤!")
            print("\nì£¼ìš” ë°œê²¬:")
            for name, corr, p_val in basic_results:
                if p_val < 0.05:
                    direction = "ì–‘ì˜" if corr > 0 else "ìŒì˜"
                    strength = "ê°•í•œ" if abs(corr) > 0.7 else "ì¤‘ê°„" if abs(corr) > 0.4 else "ì•½í•œ"
                    print(f"  â€¢ {name}: {direction} {strength} ìƒê´€ê´€ê³„ (r={corr:+.4f}, p={p_val:.4f})")
            
            print("\nğŸ’¡ ì˜ë¯¸:")
            print("  â†’ í…”ë ˆê·¸ë¨ ì»¤ë®¤ë‹ˆí‹° í™œë™ì„ ëª¨ë‹ˆí„°ë§í•˜ë©´ ê°€ê²©/ê±°ë˜ ë³€í™” ì˜ˆì¸¡ ê°€ëŠ¥")
            print("  â†’ ìŠ¤íŒŒì´í¬ ì•ŒëŒ ì‹œìŠ¤í…œìœ¼ë¡œ ì‹¤ì‹œê°„ ê°ì§€ ì¶”ì²œ")
        
        elif significant_count == 1:
            print("\nâš ï¸ ê²°ë¡ : ì¼ë¶€ ìœ ì˜ë¯¸í•œ ìƒê´€ê´€ê³„ê°€ ë°œê²¬ë˜ì—ˆìœ¼ë‚˜ ì „ë°˜ì ìœ¼ë¡œ ì•½í•©ë‹ˆë‹¤.")
            print("\nê°€ëŠ¥í•œ ì´ìœ :")
            print("  â€¢ í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ìˆ˜ê°€ ì ìŒ (ë” í™œë°œí•œ ì±„ë„ í•„ìš”)")
            print("  â€¢ ì‹œì°¨ íš¨ê³¼ (Lag Correlation í™•ì¸)")
            print("  â€¢ ë°ì´í„° ìˆ˜ì§‘ ê¸°ê°„ í™•ì¥ í•„ìš”")
        
        else:
            print("\nâŒ ê²°ë¡ : í…”ë ˆê·¸ë¨ í™œë™ê³¼ ì‹œì¥ ì§€í‘œ ê°„ ìœ ì˜ë¯¸í•œ ì§ì ‘ ìƒê´€ê´€ê³„ê°€ ì—†ìŠµë‹ˆë‹¤.")
            print("\nê°€ëŠ¥í•œ ì´ìœ :")
            print("  â€¢ ë°ì´í„° ìˆ˜ì§‘ ê¸°ê°„ì´ ì§§ìŒ")
            print("  â€¢ í…”ë ˆê·¸ë¨ ì±„ë„ì´ ì‹œì¥ì— ì˜í–¥ë ¥ì´ ì ìŒ")
            print("  â€¢ ì‹œì°¨ê°€ ì¡´ì¬ (Lag Correlationì´ë‚˜ Granger Causality í™•ì¸)")
            print("  â€¢ ë¹„ì„ í˜• ê´€ê³„ì¼ ìˆ˜ ìˆìŒ (Spearman ìƒê´€ê³„ìˆ˜ í™•ì¸)")


def save_results_to_file(df, results):
    """ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
    output_path = '/Volumes/T7/class/2025-FALL/big_data/data/telegram_correlation_analysis.csv'
    
    # ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±
    df_corr = df[['message_count', 'ETH_close', 'BTC_close', 'tx_frequency', 'tx_amount', 'avg_sentiment']].corr()
    df_corr.to_csv(output_path)
    
    print(f"\nğŸ’¾ ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_path}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("\n" + "=" * 80)
    print("ğŸ” í…”ë ˆê·¸ë¨ ì»¤ë®¤ë‹ˆí‹° í™œë™ ìƒê´€ê´€ê³„ ë¶„ì„")
    print("=" * 80)
    
    # ë°ì´í„° ë¡œë“œ
    print("\nğŸ“‚ ë°ì´í„° ë¡œë”© ì¤‘...")
    df = load_data()
    print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)} ì‹œê°„")
    
    # 1. ê¸°ë³¸ ìƒê´€ê´€ê³„
    basic_results = analyze_basic_correlation(df)
    
    # 2. ì‹œì°¨ ìƒê´€ê´€ê³„
    analyze_lag_correlation(df)
    
    # 3. ë³€í™”ìœ¨ ìƒê´€ê´€ê³„
    analyze_change_correlation(df)
    
    # 4. ê·¸ëœì € ì¸ê³¼ê´€ê³„
    analyze_granger_causality(df)
    
    # 5. ì¢…í•© ìš”ì•½
    generate_summary(df, basic_results)
    
    # 6. ê²°ê³¼ ì €ì¥
    save_results_to_file(df, basic_results)
    
    print("\n" + "=" * 80)
    print("âœ… ë¶„ì„ ì™„ë£Œ!")
    print("=" * 80)
    print("\nğŸ’¡ ëŒ€ì‹œë³´ë“œì—ì„œ ë” ìì„¸í•œ ì‹œê°í™”ë¥¼ í™•ì¸í•˜ì„¸ìš”:")
    print("   streamlit run app.py")
    print("\n")


if __name__ == '__main__':
    main()





