"""
ëª¨ë“  ë‰´ìŠ¤ ë°ì´í„° íŒŒì¼ì„ ìƒˆë¡œ ìˆ˜ì§‘í•œ ë°ì´í„°ë¡œ ì—…ë°ì´íŠ¸

ìƒˆë¡œ ìˆ˜ì§‘í•œ coinness_data2.csvë¥¼ ê¸°ì¤€ìœ¼ë¡œ ëª¨ë“  ë‰´ìŠ¤ ë°ì´í„° íŒŒì¼ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
"""

import pandas as pd
import os
from datetime import datetime

def update_all_news_data():
    """ëª¨ë“  ë‰´ìŠ¤ ë°ì´í„° ì—…ë°ì´íŠ¸"""
    print("=" * 70)
    print("ëª¨ë“  ë‰´ìŠ¤ ë°ì´í„° ì—…ë°ì´íŠ¸")
    print("=" * 70)
    
    # íŒŒì¼ ê²½ë¡œ
    data_dir = 'data'
    new_data_file = os.path.join(data_dir, 'coinness_data2.csv')
    main_file = os.path.join(data_dir, 'coinness_data.csv')
    
    # ìƒˆë¡œ ìˆ˜ì§‘í•œ ë°ì´í„° ë¡œë“œ
    if not os.path.exists(new_data_file):
        print(f"âŒ ìƒˆë¡œ ìˆ˜ì§‘í•œ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {new_data_file}")
        return
    
    print(f"\nğŸ“¥ ìƒˆë¡œ ìˆ˜ì§‘í•œ ë°ì´í„° ë¡œë“œ: {new_data_file}")
    df_new = pd.read_csv(new_data_file)
    df_new['timestamp'] = pd.to_datetime(df_new['timestamp'], errors='coerce')
    df_new = df_new.dropna(subset=['timestamp'])
    
    print(f"   ìƒˆë¡œ ìˆ˜ì§‘í•œ ë°ì´í„°: {len(df_new):,}ê°œ ê¸°ì‚¬")
    print(f"   ê¸°ê°„: {df_new['timestamp'].min()} ~ {df_new['timestamp'].max()}")
    
    # ê¸°ì¡´ coinness_data.csvê°€ ìˆìœ¼ë©´ ë³‘í•© (ì¤‘ë³µ ì œê±°)
    if os.path.exists(main_file):
        print(f"\nğŸ“¥ ê¸°ì¡´ ë°ì´í„° ë¡œë“œ: {main_file}")
        df_existing = pd.read_csv(main_file)
        df_existing['timestamp'] = pd.to_datetime(df_existing['timestamp'], errors='coerce')
        df_existing = df_existing.dropna(subset=['timestamp'])
        print(f"   ê¸°ì¡´ ë°ì´í„°: {len(df_existing):,}ê°œ ê¸°ì‚¬")
        print(f"   ê¸°ê°„: {df_existing['timestamp'].min()} ~ {df_existing['timestamp'].max()}")
        
        # ë³‘í•© (ìƒˆ ë°ì´í„° ìš°ì„ )
        print(f"\nğŸ”„ ë°ì´í„° ë³‘í•© ì¤‘...")
        df_combined = pd.concat([df_existing, df_new], ignore_index=True)
        
        # ì¤‘ë³µ ì œê±° (link ê¸°ì¤€, ìƒˆ ë°ì´í„° ìš°ì„ )
        if 'link' in df_combined.columns:
            before = len(df_combined)
            df_combined = df_combined.drop_duplicates(subset=['link'], keep='last')
            duplicates_removed = before - len(df_combined)
            print(f"   ì¤‘ë³µ ì œê±°: {duplicates_removed:,}ê°œ (link ê¸°ì¤€)")
        
        # title + timestamp ê¸°ì¤€ìœ¼ë¡œë„ ì¤‘ë³µ ì œê±°
        if 'title' in df_combined.columns:
            before = len(df_combined)
            df_combined = df_combined.drop_duplicates(
                subset=['title', 'timestamp'], 
                keep='last'
            )
            duplicates_removed = before - len(df_combined)
            if duplicates_removed > 0:
                print(f"   ì¶”ê°€ ì¤‘ë³µ ì œê±°: {duplicates_removed:,}ê°œ (title+timestamp ê¸°ì¤€)")
    else:
        print(f"\nâš ï¸  ê¸°ì¡´ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ìƒˆ ë°ì´í„°ë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
        df_combined = df_new.copy()
    
    # timestamp ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ìµœì‹ ìˆœ)
    df_combined = df_combined.sort_values('timestamp', ascending=False).reset_index(drop=True)
    
    print(f"\nâœ… ë³‘í•© ì™„ë£Œ: ì´ {len(df_combined):,}ê°œ ê¸°ì‚¬")
    print(f"   ê¸°ê°„: {df_combined['timestamp'].min()} ~ {df_combined['timestamp'].max()}")
    
    # ë°ì´í„° ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(data_dir, exist_ok=True)
    
    # 1. coinness_data.csv ì—…ë°ì´íŠ¸ (Streamlit ëŒ€ì‹œë³´ë“œìš©)
    print(f"\nğŸ’¾ ì €ì¥ ì¤‘: {main_file}")
    df_combined.to_csv(main_file, index=False, encoding='utf-8-sig')
    print(f"   âœ… ì €ì¥ ì™„ë£Œ: {main_file}")
    
    # 2. coinness_data2.csv ì—…ë°ì´íŠ¸ (Next.js ëŒ€ì‹œë³´ë“œìš©) - ì´ë¯¸ ìµœì‹ ì´ì§€ë§Œ í™•ì‹¤íˆ í•˜ê¸° ìœ„í•´
    print(f"\nğŸ’¾ ì €ì¥ ì¤‘: {new_data_file}")
    df_combined.to_csv(new_data_file, index=False, encoding='utf-8-sig')
    print(f"   âœ… ì €ì¥ ì™„ë£Œ: {new_data_file}")
    
    # í†µê³„ ì¶œë ¥
    print(f"\nğŸ“Š ì—…ë°ì´íŠ¸ í†µê³„:")
    print(f"   ì´ ê¸°ì‚¬ ìˆ˜: {len(df_combined):,}ê°œ")
    if 'sentiment_compound' in df_combined.columns:
        print(f"   í‰ê·  ê°ì • ì ìˆ˜: {df_combined['sentiment_compound'].mean():.3f}")
        pos = (df_combined['sentiment_compound'] > 0.05).sum()
        neg = (df_combined['sentiment_compound'] < -0.05).sum()
        neu = len(df_combined) - pos - neg
        print(f"   ê¸ì • ë¹„ìœ¨: {pos:,}ê°œ ({pos/len(df_combined)*100:.1f}%)")
        print(f"   ë¶€ì • ë¹„ìœ¨: {neg:,}ê°œ ({neg/len(df_combined)*100:.1f}%)")
        print(f"   ì¤‘ë¦½ ë¹„ìœ¨: {neu:,}ê°œ ({neu/len(df_combined)*100:.1f}%)")
    
    # ìµœê·¼ 7ì¼ ë°ì´í„° í†µê³„
    seven_days_ago = datetime.now() - pd.Timedelta(days=7)
    df_recent_7d = df_combined[df_combined['timestamp'] >= seven_days_ago]
    print(f"\nğŸ“… ìµœê·¼ 7ì¼ í†µê³„:")
    print(f"   ê¸°ì‚¬ ìˆ˜: {len(df_recent_7d):,}ê°œ")
    if len(df_recent_7d) > 0 and 'sentiment_compound' in df_recent_7d.columns:
        print(f"   í‰ê·  ê°ì • ì ìˆ˜: {df_recent_7d['sentiment_compound'].mean():.3f}")
    
    # ì›”ë³„ í†µê³„
    if not df_combined.empty:
        print(f"\nğŸ“… ì›”ë³„ ê¸°ì‚¬ ìˆ˜ (ìµœê·¼ 6ê°œì›”):")
        monthly = df_combined.groupby(df_combined['timestamp'].dt.to_period('M')).size().sort_index(ascending=False)
        for month, count in monthly.head(6).items():
            print(f"   {month}: {count:,}ê°œ")
    
    print(f"\nâœ… ëª¨ë“  ë‰´ìŠ¤ ë°ì´í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ!")
    print(f"\nğŸ“ ì—…ë°ì´íŠ¸ëœ íŒŒì¼:")
    print(f"   - {main_file} (Streamlit ëŒ€ì‹œë³´ë“œìš©)")
    print(f"   - {new_data_file} (Next.js ëŒ€ì‹œë³´ë“œìš©)")


if __name__ == '__main__':
    update_all_news_data()

